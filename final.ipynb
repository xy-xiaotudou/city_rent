{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 计算是2018年的第几天\n",
    "def is_leap_year(year):\n",
    "    if year % 4 == 0 and year % 100 != 0 or year % 400 == 0:\n",
    "        return True\n",
    "    return False\n",
    "def which_day(df):\n",
    "    year = int(df.split('/')[0])\n",
    "    month = int(df.split('/')[1])\n",
    "    day = int(df.split('/')[2])\n",
    "    month_of_day31 = [1,3,5,7,8,10,12]\n",
    "    month_of_day30 = [4,6,9,11]\n",
    "    feb_month = 2\n",
    "    if month ==1 :\n",
    "        return day\n",
    "    if month == 2:\n",
    "        return day + 31\n",
    "    days_of_31_num = 0\n",
    "    days_of_30_num = 0\n",
    "    ## 31天月份数\n",
    "    for days_of_31 in month_of_day31:\n",
    "        if days_of_31 < month:\n",
    "            days_of_31_num += 1\n",
    "        else:\n",
    "            break\n",
    "    ## 30天月份数\n",
    "    for days_of_30 in month_of_day31:\n",
    "        if days_of_30 < month:\n",
    "            days_of_30_num += 1\n",
    "        else:\n",
    "            break\n",
    "    return days_of_31_num * 31 + days_of_30_num * 30 +(29 if is_leap_year(year) else 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(data,is_test=True):       \n",
    "    # buildYear 暂无信息 处理(众数填充)\n",
    "    tmp = data['buildYear'].copy()\n",
    "    tmp2 = tmp[tmp!='暂无信息'].astype('int')\n",
    "    tmp[tmp=='暂无信息'] = tmp2.mode()[0]\n",
    "    data['buildYear'] = tmp\n",
    "    data['buildYear'] = data['buildYear'].astype('int')\n",
    "    \n",
    "    # 缺失值处理\n",
    "    data['pv'].replace(to_replace=np.nan,value=data['pv'].mean(), inplace=True)\n",
    "    data['uv'].replace(to_replace=np.nan,value=data['uv'].mean(), inplace=True)\n",
    "\n",
    " \n",
    "    # 拆分 houseType\n",
    "    data[['bedroom', 'hall', 'wc', 'null']] = data['houseType'].str.split('[室厅卫]', expand=True)\n",
    "    data[['bedroom', 'hall', 'wc']] = data[['bedroom', 'hall', 'wc']].astype('int64')\n",
    "    data.drop(['null','houseType'], axis=1, inplace=True)\n",
    "    # rentType未知方式 处理\n",
    "    data.loc[(data['rentType']=='--','rentType')] = '未知方式'\n",
    "\n",
    "    \n",
    "    # 拆分 tradeTime \n",
    "    data['tradeMonth'] = data['tradeTime'].apply(lambda x:x.split('/')[1]).astype('int64')\n",
    "    data['day'] = data['tradeTime'].apply(lambda x: int(x.split('/')[1])).astype('int64') ##将时间特征进行放大  \n",
    "    ### 看看交易时间是2018年的第几天\n",
    "    data['whichday'] = data['tradeTime'].apply(which_day)\n",
    "    data['whichday'] = data['whichday'].astype('int64')\n",
    "    data = data.drop(['tradeTime'],axis=1)\n",
    "    if is_test:\n",
    "        # 去除ID列\n",
    "        data.drop(['ID'], axis=1, inplace=True)\n",
    "    # 去除部分特征\n",
    "    data.drop(['city'], axis=1, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Data\n",
    "def cleanData(data):    \n",
    "    data.drop(data[(data['tradeMoney']>16000)].index,inplace=True) \n",
    "    data.drop(data[(data['area']>160)].index,inplace=True)\n",
    "    data.drop(data[(data['tradeMoney']<800)].index,inplace=True)\n",
    "    data.drop(data[(data['totalFloor']==0)].index,inplace=True)\n",
    "\n",
    "    # 深度清理\n",
    "    data.drop(data[(data['region']=='RG00001')&(data['tradeMoney']<1000)&(data['area']>50)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00001') & (data['tradeMoney']>25000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00001') & (data['area']>250)&(data['tradeMoney']<20000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00001') & (data['area']>400)&(data['tradeMoney']>50000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00001') & (data['area']>100)&(data['tradeMoney']<2000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00002') & (data['area']<100)&(data['tradeMoney']>60000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00003') & (data['area']<300)&(data['tradeMoney']>30000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00003')&(data['tradeMoney']<500)&(data['area']<50)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00003')&(data['tradeMoney']<1500)&(data['area']>100)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00003')&(data['tradeMoney']<2000)&(data['area']>300)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00003')&(data['tradeMoney']>5000)&(data['area']<20)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00003') & (data['area']>600)&(data['tradeMoney']>40000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00004')&(data['tradeMoney']<1000)&(data['area']>80)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00006') & (data['tradeMoney']<200)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00005')&(data['tradeMoney']<2000)&(data['area']>180)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00005')&(data['tradeMoney']>50000)&(data['area']<200)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00006') & (data['area']>200)&(data['tradeMoney']<2000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00007') & (data['area']>100)&(data['tradeMoney']<2500)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00010') & (data['area']>200)&(data['tradeMoney']>25000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00010') & (data['area']>400)&(data['tradeMoney']<15000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00010')&(data['tradeMoney']<3000)&(data['area']>200)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00010')&(data['tradeMoney']>7000)&(data['area']<75)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00010')&(data['tradeMoney']>12500)&(data['area']<100)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00004') & (data['area']>400)&(data['tradeMoney']>20000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00008')&(data['tradeMoney']<2000)&(data['area']>80)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00009') & (data['tradeMoney']>40000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00009') & (data['area']>300)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00009')&(data['area']>100)&(data['tradeMoney']<2000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00011')&(data['tradeMoney']<10000)&(data['area']>390)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00012') & (data['area']>120)&(data['tradeMoney']<5000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00013') & (data['area']<100)&(data['tradeMoney']>40000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00013') & (data['area']>400)&(data['tradeMoney']>50000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00013')&(data['area']>80)&(data['tradeMoney']<2000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00014') & (data['area']>300)&(data['tradeMoney']>40000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00014')&(data['tradeMoney']<1300)&(data['area']>80)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00014')&(data['tradeMoney']<8000)&(data['area']>200)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00014')&(data['tradeMoney']<1000)&(data['area']>20)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00014')&(data['tradeMoney']>25000)&(data['area']>200)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00014')&(data['tradeMoney']<20000)&(data['area']>250)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00005')&(data['tradeMoney']>30000)&(data['area']<100)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00005')&(data['tradeMoney']<50000)&(data['area']>600)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00005')&(data['tradeMoney']>50000)&(data['area']>350)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00006')&(data['tradeMoney']>4000)&(data['area']<100)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00006')&(data['tradeMoney']<600)&(data['area']>100)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00006')&(data['area']>165)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00012')&(data['tradeMoney']<800)&(data['area']<30)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00007')&(data['tradeMoney']<1100)&(data['area']>50)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00004')&(data['tradeMoney']>8000)&(data['area']<80)].index,inplace=True)\n",
    "    data.loc[(data['region']=='RG00002')&(data['area']>50)&(data['rentType']=='合租'),'rentType']='整租'\n",
    "    data.loc[(data['region']=='RG00014')&(data['rentType']=='合租')&(data['area']>60),'rentType']='整租'\n",
    "    data.drop(data[(data['region']=='RG00008')&(data['tradeMoney']>15000)&(data['area']<110)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00008')&(data['tradeMoney']>20000)&(data['area']>110)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00008')&(data['tradeMoney']<1500)&(data['area']<50)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00008')&(data['rentType']=='合租')&(data['area']>50)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00015') ].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00014')&(data['tradeMoney']>13000)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00005')&(data['tradeMoney']<1200)&(data['area']>80)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00005')&(data['tradeMoney']>13000)&(data['area']<90)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00005')&(data['tradeMoney']<1600)&(data['area']>100)].index,inplace=True)\n",
    "    data.drop(data[(data['region']=='RG00002')&(data['tradeMoney']>13000)&(data['area']<100)].index,inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Features\n",
    "def MakeFeatures_1(data):\n",
    "    # 合并车站\n",
    "    data['bus_sub_num'] = data['subwayStationNum']+data['busStationNum']\n",
    "    # 合并学校\n",
    "    data['school_num'] = data['interSchoolNum']+data['schoolNum']+data['privateSchoolNum']\n",
    "    # 合并医疗\n",
    "    data['help_sum'] = data['hospitalNum']+data['drugStoreNum']\n",
    "    # 合并生活设施\n",
    "    data['play_sum'] = data['gymNum']+data['parkNum']+data['bankNum']\n",
    "    # 合并购物\n",
    "    data['shop_num'] = data['shopNum']+data['mallNum']+data['superMarketNum']\n",
    "    # 其他合并\n",
    "    data['totalNewTradeMoney_Workers'] = data['totalNewTradeMoney'] + data['totalWorkers']\n",
    "    data['bankNum_Workers'] = data['bankNum'] + data['totalWorkers']\n",
    "    data['gym_bankNum'] = data['bankNum'] + data['gymNum']\n",
    "    # \"板块二手房价\"\n",
    "    data['area_mean_price'] = (data['area']*data['tradeMeanPrice'])/1000\n",
    "    # \"板块新房房价\"\n",
    "    data['New_area_mean_price'] = (data['area']*data['tradeNewMeanPrice'])/1000\n",
    "    # \"板块房价\"\n",
    "#     data['Mean_price'] = (data['tradeMeanPrice']+data['tradeNewMeanPrice'])/2 #变差\n",
    "#     data['Mean_area_mean_price'] = (data['area']*data['Mean_price'])/1000 #变差\n",
    "############### Hero 特征 ###################################################################\n",
    "    def get_train_mode(train, data, by, fea):\n",
    "        def Encode_houseFloor(x):\n",
    "            if x =='低':\n",
    "                return 1\n",
    "            if x =='中':\n",
    "                return 2\n",
    "            if x =='高':\n",
    "                return 3\n",
    "        if fea=='area':\n",
    "            train[fea] = train[fea].apply(lambda x: round(x,-1))\n",
    "        if fea=='tradeMoney':\n",
    "            train[fea] = train[fea].apply(lambda x: round(x,-2)) \n",
    "        if fea=='houseFloor':\n",
    "            train[fea] = train[fea].apply(Encode_houseFloor)\n",
    "        gp = train.groupby(by)[fea].agg(lambda x: np.mean(pd.Series.mode(x))).reset_index()     \n",
    "        gp = gp.rename(columns={fea:f'trn_{\"_\".join(by)}_{fea}_mode'})\n",
    "        data = pd.merge(data, gp, how='left', on=by)\n",
    "        return data \n",
    "    def get_train_mean(train, data, by, fea):\n",
    "        gp = train.groupby(by)[fea].mean().reset_index()\n",
    "        gp = gp.rename(columns={fea:f'trn_{\"_\".join(by)}_{fea}_mean'})\n",
    "        data = pd.merge(data, gp, how='left', on=by)\n",
    "        return data          \n",
    "    def get_train_median(train, data, by, fea):\n",
    "        gp = train.groupby(by)[fea].median().reset_index()\n",
    "        gp = gp.rename(columns={fea:f'trn_{\"_\".join(by)}_{fea}_median'})\n",
    "        data = pd.merge(data, gp, how='left', on=by)\n",
    "        return data \n",
    "    def get_train_std(train, data, by, fea):\n",
    "        gp = train.groupby(by)[fea].std().reset_index()\n",
    "        gp = gp.rename(columns={fea:f'trn_{\"_\".join(by)}_{fea}_std'})\n",
    "        data = pd.merge(data, gp, how='left', on=by)\n",
    "        data[f'trn_{\"_\".join(by)}_{fea}_std'] = data[f'trn_{\"_\".join(by)}_{fea}_std'].fillna(0)\n",
    "        return data\n",
    "    def get_train_max(train, data, by, fea):\n",
    "        gp = train.groupby(by)[fea].max().reset_index()\n",
    "        gp = gp.rename(columns={fea:f'trn_{\"_\".join(by)}_{fea}_max'})\n",
    "        data = pd.merge(data, gp, how='left', on=by)\n",
    "        return data\n",
    "    def get_train_min(train, data, by, fea):\n",
    "        gp = train.groupby(by)[fea].min().reset_index()\n",
    "        gp = gp.rename(columns={fea:f'trn_{\"_\".join(by)}_{fea}_min'})\n",
    "        data = pd.merge(data, gp, how='left', on=by)\n",
    "        return data\n",
    "    def groupby(data,by_list): \n",
    "        # 读取全量数据                        \n",
    "        train = pd.read_csv('./train_data.csv')\n",
    "        train = parseData(train)\n",
    "        # 数据处理\n",
    "        train.loc[(train['area']>1000),'area'] = 1000\n",
    "        train['area_mean_price'] = (train['area']*train['tradeMeanPrice'])/1000  \n",
    "        train['New_area_mean_price'] = (train['area']*train['tradeNewMeanPrice'])/1000\n",
    "#         # \"板块房价\"\n",
    "#         train['Mean_price'] = (train['tradeMeanPrice']+train['tradeNewMeanPrice'])/2\n",
    "#         train['Mean_area_mean_price'] = (train['area']*train['Mean_price'])/1000                        \n",
    "#         train['pv_uv_ratio'] = train['pv']/(train['uv']+1) \n",
    "        # 对 area_mean_price统计                       \n",
    "#         data = get_train_mode(train, data, by, 'area_mean_price')\n",
    "        data = get_train_mean(train, data, by, 'area_mean_price')\n",
    "        data = get_train_std(train, data, by, 'area_mean_price')\n",
    "#         data = get_train_median(train, data, by, 'area_mean_price')\n",
    "#         # 对 New_area_mean_price统计\n",
    "        data = get_train_mean(train, data, by, 'New_area_mean_price')\n",
    "        data = get_train_std(train, data, by, 'New_area_mean_price')\n",
    "#         data = get_train_mean(train, data, by, 'Mean_price')\n",
    "#         data = get_train_std(train, data, by, 'Mean_price')                        \n",
    "#         data = get_train_mean(train, data, by, 'Mean_area_mean_price')\n",
    "#         data = get_train_std(train, data, by, 'Mean_area_mean_price')                        \n",
    "                                \n",
    "#         # 对pv_uv_ratio统计\n",
    "#         data = get_train_mode(train, data, by, 'pv_uv_ratio')\n",
    "#         data = get_train_mean(train, data, by, 'pv_uv_ratio')\n",
    "#         data = get_train_std(train, data, by, 'pv_uv_ratio')\n",
    "#         data = get_train_median(train, data, by, 'pv_uv_ratio')                        \n",
    "#         # 对面积做统计特征，（有用）\n",
    "        data = get_train_mode(train, data, by, 'area')\n",
    "#         data = get_train_mean(train, data, by, 'area')\n",
    "        data = get_train_std(train, data, by, 'area')\n",
    "        data = get_train_median(train, data, by, 'area')                                                       \n",
    "#     #     data = get_train_max(train, data, by, 'area') #加了过拟合\n",
    "#     #     data = get_train_min(train, data, by, 'area') #加了过拟合\n",
    "#         # 众数                            \n",
    "#         data = get_train_mode(train, data, by, 'tradeMonth') #加了变差\n",
    "#         data = get_train_mode(train, data, by, 'houseFloor') #加了变差                           \n",
    "#         data = get_train_mode(train, data, by, 'buildYear') #加了变差\n",
    "#         data = get_train_mode(train, data, by, 'totalFloor')\n",
    "        data = get_train_mode(train, data, by, 'pv')\n",
    "#         data = get_train_mode(train, data, by, 'uv')\n",
    "        data = get_train_mode(train, data, by, 'tradeMeanPrice')\n",
    "        data = get_train_mode(train, data, by, 'tradeNewMeanPrice')                               \n",
    "#         data = get_train_mode(train, data, by, 'totalTradeMoney')\n",
    "        data = get_train_mode(train, data, by, 'totalNewTradeMoney')\n",
    "        data = get_train_mode(train, data, by, 'saleSecHouseNum')\n",
    "        data = get_train_mode(train, data, by, 'totalTradeArea')\n",
    "#         data = get_train_mode(train, data, by, 'tradeSecNum')\n",
    "#         data = get_train_mode(train, data, by, 'totalNewTradeArea') \n",
    "#         data = get_train_mode(train, data, by, 'tradeNewNum')\n",
    "        data = get_train_mode(train, data, by, 'remainNewNum')\n",
    "        data = get_train_mode(train, data, by, 'supplyNewNum')                           \n",
    "        data = get_train_mode(train, data, by, 'newWorkers')  \n",
    "        data = get_train_mode(train, data, by, 'bedroom')\n",
    "#         data = get_train_mode(train, data, by, 'totalWorkers')                            \n",
    "#         # 均值                            \n",
    "#         data = get_train_mean(train, data, by, 'tradeMonth') #加了变差\n",
    "#         data = get_train_mean(train, data, by, 'houseFloor') #加了变差                           \n",
    "#         data = get_train_mean(train, data, by, 'buildYear') #加了变差\n",
    "#         data = get_train_mean(train, data, by, 'totalFloor')\n",
    "#         data = get_train_mean(train, data, by, 'pv')\n",
    "#         data = get_train_mean(train, data, by, 'uv')\n",
    "#         data = get_train_mean(train, data, by, 'tradeMeanPrice')\n",
    "#         data = get_train_mean(train, data, by, 'tradeNewMeanPrice')                            \n",
    "#         data = get_train_mean(train, data, by, 'totalTradeMoney')\n",
    "#         data = get_train_mean(train, data, by, 'totalNewTradeMoney')\n",
    "        data = get_train_mean(train, data, by, 'saleSecHouseNum')\n",
    "#         data = get_train_mean(train, data, by, 'totalTradeArea')\n",
    "#         data = get_train_mean(train, data, by, 'tradeSecNum')\n",
    "#         data = get_train_mean(train, data, by, 'totalNewTradeArea') \n",
    "#         data = get_train_mean(train, data, by, 'tradeNewNum')\n",
    "#         data = get_train_mean(train, data, by, 'remainNewNum')\n",
    "        data = get_train_mean(train, data, by, 'supplyNewNum')\n",
    "        data = get_train_mean(train, data, by, 'newWorkers')\n",
    "#         data = get_train_mean(train, data, by, 'bedroom')\n",
    "#         data = get_train_mean(train, data, by, 'totalWorkers') # 特征选择删除了    \n",
    "#         # 方差\n",
    "#         data = get_train_std(train, data, by, 'tradeMonth') #加了变差\n",
    "#         data = get_train_std(train, data, by, 'houseFloor') #加了变差                           \n",
    "#         data = get_train_std(train, data, by, 'buildYear') #加了变差                            \n",
    "        data = get_train_std(train, data, by, 'totalFloor')\n",
    "        data = get_train_std(train, data, by, 'pv')\n",
    "#         data = get_train_std(train, data, by, 'uv')\n",
    "        data = get_train_std(train, data, by, 'tradeMeanPrice')\n",
    "        data = get_train_std(train, data, by, 'tradeNewMeanPrice')                               \n",
    "        data = get_train_std(train, data, by, 'totalTradeMoney')\n",
    "        data = get_train_std(train, data, by, 'totalNewTradeMoney')\n",
    "#         data = get_train_std(train, data, by, 'saleSecHouseNum')\n",
    "        data = get_train_std(train, data, by, 'totalTradeArea')\n",
    "        data = get_train_std(train, data, by, 'tradeSecNum')\n",
    "#         data = get_train_std(train, data, by, 'totalNewTradeArea') \n",
    "#         data = get_train_std(train, data, by, 'tradeNewNum')\n",
    "        data = get_train_std(train, data, by, 'remainNewNum')\n",
    "        data = get_train_std(train, data, by, 'supplyNewNum')\n",
    "#         data = get_train_std(train, data, by, 'newWorkers')  \n",
    "        data = get_train_std(train, data, by, 'bedroom')                                \n",
    "#         data = get_train_std(train, data, by, 'totalWorkers') # 特征选择删除了\n",
    "#         #中位数\n",
    "#         data = get_train_median(train, data, by, 'tradeMonth') #加了变差\n",
    "#         data = get_train_median(train, data, by, 'houseFloor') #加了变差                           \n",
    "#         data = get_train_median(train, data, by, 'buildYear') #加了变差                              \n",
    "#         data = get_train_median(train, data, by, 'totalFloor')\n",
    "#         data = get_train_median(train, data, by, 'pv')\n",
    "#         data = get_train_median(train, data, by, 'uv')\n",
    "#         data = get_train_median(train, data, by, 'tradeMeanPrice')\n",
    "#         data = get_train_median(train, data, by, 'tradeNewMeanPrice')                               \n",
    "        data = get_train_median(train, data, by, 'totalTradeMoney')\n",
    "        data = get_train_median(train, data, by, 'totalNewTradeMoney')\n",
    "#         data = get_train_median(train, data, by, 'saleSecHouseNum') # 特征选择删除了\n",
    "#         data = get_train_median(train, data, by, 'totalTradeArea')\n",
    "#         data = get_train_median(train, data, by, 'tradeSecNum')\n",
    "#         data = get_train_median(train, data, by, 'totalNewTradeArea') \n",
    "#         data = get_train_median(train, data, by, 'tradeNewNum')\n",
    "#         data = get_train_median(train, data, by, 'remainNewNum')\n",
    "#         data = get_train_median(train, data, by, 'supplyNewNum') # 特征选择删除了\n",
    "#         data = get_train_median(train, data, by, 'newWorkers')  \n",
    "#         data = get_train_median(train, data, by, 'bedroom')  # 特征选择删除了\n",
    "#         data = get_train_median(train, data, by, 'totalWorkers') # 特征选择删除了\n",
    "        return data                        \n",
    "####################################################################################                                \n",
    "    by=['communityName']\n",
    "    data = groupby(data, by)\n",
    "    by=['plate']\n",
    "    data = groupby(data, by)                           \n",
    "################## 平滑处理 ################################################################                                \n",
    "    big_num_cols = ['totalTradeMoney','totalTradeArea','tradeMeanPrice','totalNewTradeMoney', 'totalNewTradeArea',\n",
    "                'tradeNewMeanPrice','remainNewNum', 'supplyNewNum', 'supplyLandArea',\n",
    "                'tradeLandArea','landTotalPrice','landMeanPrice','totalWorkers','newWorkers',\n",
    "                'residentPopulation','pv','uv',\n",
    "                   ]\n",
    "    num_cols = ['area','subwayStationNum', 'busStationNum', 'interSchoolNum', 'schoolNum',\n",
    "                'privateSchoolNum', 'hospitalNum', 'drugStoreNum', 'gymNum', 'bankNum', 'shopNum',\n",
    "                'parkNum', 'mallNum', 'superMarketNum', 'totalTradeMoney', 'totalTradeArea', 'tradeMeanPrice',\n",
    "                'tradeSecNum', 'totalNewTradeMoney', 'totalNewTradeArea', 'tradeNewMeanPrice', 'tradeNewNum',\n",
    "                'remainNewNum', 'supplyNewNum', 'supplyLandNum', 'supplyLandArea', 'tradeLandNum',\n",
    "                'tradeLandArea', 'landTotalPrice', 'landMeanPrice', 'totalWorkers', 'newWorkers',\n",
    "                'residentPopulation', 'pv', 'uv', 'lookNum'] \n",
    "    for col in num_cols:\n",
    "        high = np.percentile(data[col].values, 99.8)\n",
    "        low = np.percentile(data[col].values, 0.2)\n",
    "        data.loc[data[col] > high, col] = high\n",
    "        data.loc[data[col] < low, col] = low\n",
    "    # 过大量级值取log平滑\n",
    "    for col in big_num_cols:\n",
    "        data[col] = data[col].map(lambda x: np.log1p(x))                            \n",
    "\n",
    "##################################################################################                              \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v(data):\n",
    "    print(data.shape)\n",
    "    path = './'\n",
    "    save_path = path+'w2v'\n",
    "    sentence = []\n",
    "    w2v_list = ['communityName']\n",
    "    for line in list(data[w2v_list].values):\n",
    "        sentence.append([str(l) for idx, l in enumerate(line)])\n",
    "    print('training...')\n",
    "    L=10\n",
    "    model = Word2Vec(sentence, size=L, window=2, min_count=1, \n",
    "                     workers=multiprocessing.cpu_count(),iter=10)\n",
    "    print('outputing...')\n",
    "    \n",
    "    for fea in w2v_list:\n",
    "        values = []\n",
    "        for line in list(data[fea].values):\n",
    "            values.append(line)\n",
    "        values = set(values)\n",
    "#         print(fea,len(values))\n",
    "        # 提取每个词的词向量\n",
    "        w2v = []\n",
    "        for i in values:\n",
    "            a = [i]\n",
    "            a.extend(model[str(i)])\n",
    "            w2v.append(a)\n",
    "        out_df = pd.DataFrame(w2v)\n",
    "        # 设置列名\n",
    "        name = [fea]\n",
    "        for i in range(L):\n",
    "            name.append(name[0] + 'W' + str(i))\n",
    "        out_df.columns = name\n",
    "        out_df.to_csv(save_path + '/' + fea + '.csv', index=False)\n",
    "    \n",
    "    \n",
    "    w2v_path = path+'w2v'\n",
    "    w2v_features = []\n",
    "    for col in ['communityName']:\n",
    "        df = pd.read_csv(w2v_path + '/' + col + '.csv')\n",
    "        fs = list(df)\n",
    "        fs.remove(col)\n",
    "        w2v_features += fs\n",
    "        data = pd.merge(data, df, on=col, how='left')\n",
    "    print('word2vec:')\n",
    "    print(w2v_features)                            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove(data):               \n",
    "    # 删除特征\n",
    "    remove_col = [ \n",
    "        'totalNewTradeMoney', \n",
    "        'help_sum', \n",
    "        'tradeNewNum', \n",
    "        'supplyLandArea', \n",
    "        'totalTradeMoney',\n",
    "        'landTotalPrice', \n",
    "        'uv', \n",
    "        'shop_num',\n",
    "        'shopNum', \n",
    "        'schoolNum', \n",
    "        'totalNewTradeArea', \n",
    "        'region', \n",
    "        'landMeanPrice', \n",
    "        'totalTradeArea', \n",
    "        'superMarketNum', \n",
    "        'tradeLandArea',\n",
    "        'rentType', \n",
    "        'gym_bankNum', \n",
    "        'communityName', \n",
    "        'play_sum',\n",
    "        'bus_sub_num',\n",
    "        'area_mean_price',\n",
    "        'New_area_mean_price',\n",
    "#         'Mean_area_mean_price',\n",
    "#         'Mean_price',\n",
    "                 ]   \n",
    "    data = data.drop(remove_col,axis=1) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category 处理\n",
    "def obj2cat(data):\n",
    "    # 转换object类型数据\n",
    "#     ['rentType', 'houseFloor', 'houseToward', 'houseDecoration', 'communityName', 'region', 'plate']\n",
    "    columns = [ 'rentType','houseFloor', 'houseToward', 'houseDecoration', 'communityName','region', 'plate'] # \n",
    "    for col in columns:\n",
    "        data[col] = data[col].astype('category')\n",
    "    return data\n",
    "def cat2obj(data):\n",
    "    # 转换数据\n",
    "    columns = [ 'buildYear', 'houseType', 'tradeTime', 'rentType', 'houseFloor', 'houseToward', 'houseDecoration', 'communityName', 'region', 'plate']\n",
    "    for col in columns:\n",
    "        data[col] = data[col].astype('object')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "def getData(is_test=True,x_train=None,x_val=None,y_train=None):\n",
    "    ### Load data ###\n",
    "    if is_test:\n",
    "        train = pd.read_csv('./train_data.csv')\n",
    "        test = pd.read_csv('./test_b.csv')\n",
    "    else:\n",
    "        train = pd.concat([x_train,y_train],axis=1)\n",
    "        test = x_val.copy()\n",
    "        train = cat2obj(train)\n",
    "        test = cat2obj(test)        \n",
    "    print('### Before data preprocessing:')\n",
    "    print(\"Train: \", train.shape[0], \"samples, and \", train.shape[1], \"cols\")\n",
    "    print(\"Test: \", test.shape[0], \"samples, and \", test.shape[1], \"cols\")\n",
    "\n",
    "    ### Data processing ###\n",
    "    train = parseData(train,is_test)\n",
    "    test = parseData(test,is_test)\n",
    "\n",
    "    train = cleanData(train)\n",
    "\n",
    "    train = MakeFeatures_1(train)\n",
    "    test = MakeFeatures_1(test)\n",
    "                        \n",
    "    train = w2v(train)\n",
    "    test = w2v(test)                            \n",
    "                       \n",
    "    train = obj2cat(train)\n",
    "    test = obj2cat(test)\n",
    "#     train,test = overfit(train,test)\n",
    "    train = remove(train)\n",
    "    test = remove(test)\n",
    "    print('### After data preprocessing:')\n",
    "    X = train.drop(['tradeMoney'],axis=1)\n",
    "    y = train['tradeMoney'].copy()\n",
    "    \n",
    "    X_test = test.copy()\n",
    "\n",
    "    print(\"Train: \", X.shape[0], \"samples, and \", X.shape[1], \"features\")\n",
    "    print(\"Test: \", X_test.shape[0], \"samples, and \", X_test.shape[1], \"features\")\n",
    "    print('Features:', X.columns)\n",
    "    return X,y,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Before data preprocessing:\n",
      "Train:  41440 samples, and  51 cols\n",
      "Test:  2401 samples, and  50 cols\n",
      "(39616, 129)\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\tools\\anaconda3.5.0\\envs\\house\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\tools\\anaconda3.5.0\\envs\\house\\lib\\site-packages\\ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec:\n",
      "['communityNameW0', 'communityNameW1', 'communityNameW2', 'communityNameW3', 'communityNameW4', 'communityNameW5', 'communityNameW6', 'communityNameW7', 'communityNameW8', 'communityNameW9']\n",
      "(2401, 128)\n",
      "training...\n",
      "outputing...\n",
      "word2vec:\n",
      "['communityNameW0', 'communityNameW1', 'communityNameW2', 'communityNameW3', 'communityNameW4', 'communityNameW5', 'communityNameW6', 'communityNameW7', 'communityNameW8', 'communityNameW9']\n",
      "### After data preprocessing:\n",
      "Train:  39616 samples, and  115 features\n",
      "Test:  2401 samples, and  115 features\n",
      "Features: Index(['area', 'houseFloor', 'totalFloor', 'houseToward', 'houseDecoration',\n",
      "       'plate', 'buildYear', 'saleSecHouseNum', 'subwayStationNum',\n",
      "       'busStationNum',\n",
      "       ...\n",
      "       'communityNameW0', 'communityNameW1', 'communityNameW2',\n",
      "       'communityNameW3', 'communityNameW4', 'communityNameW5',\n",
      "       'communityNameW6', 'communityNameW7', 'communityNameW8',\n",
      "       'communityNameW9'],\n",
      "      dtype='object', length=115)\n"
     ]
    }
   ],
   "source": [
    "X_train,Y_train,X_test = getData(is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2019-08-06 19:20:46] Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\tools\\anaconda3.5.0\\envs\\house\\lib\\site-packages\\lightgbm\\basic.py:681: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 895.868\tvalid_1's rmse: 914.838\n",
      "[400]\ttraining's rmse: 726.431\tvalid_1's rmse: 760.328\n",
      "[600]\ttraining's rmse: 668.881\tvalid_1's rmse: 720.066\n",
      "[800]\ttraining's rmse: 633.471\tvalid_1's rmse: 698.731\n",
      "[1000]\ttraining's rmse: 608.662\tvalid_1's rmse: 685.726\n",
      "[1200]\ttraining's rmse: 589.079\tvalid_1's rmse: 676.759\n",
      "[1400]\ttraining's rmse: 572.796\tvalid_1's rmse: 670.32\n",
      "[1600]\ttraining's rmse: 558.78\tvalid_1's rmse: 665.059\n",
      "[1800]\ttraining's rmse: 546.463\tvalid_1's rmse: 661.028\n",
      "[2000]\ttraining's rmse: 535.596\tvalid_1's rmse: 658.063\n",
      "[2200]\ttraining's rmse: 525.785\tvalid_1's rmse: 655.486\n",
      "[2400]\ttraining's rmse: 516.762\tvalid_1's rmse: 653.695\n",
      "[2600]\ttraining's rmse: 508.402\tvalid_1's rmse: 652.085\n",
      "[2800]\ttraining's rmse: 500.615\tvalid_1's rmse: 650.528\n",
      "[3000]\ttraining's rmse: 493.191\tvalid_1's rmse: 649.688\n",
      "[3200]\ttraining's rmse: 486.286\tvalid_1's rmse: 648.658\n",
      "[3400]\ttraining's rmse: 479.653\tvalid_1's rmse: 647.757\n",
      "[3600]\ttraining's rmse: 473.349\tvalid_1's rmse: 647.069\n",
      "[3800]\ttraining's rmse: 467.472\tvalid_1's rmse: 646.646\n",
      "[4000]\ttraining's rmse: 461.994\tvalid_1's rmse: 646.267\n",
      "[4200]\ttraining's rmse: 456.552\tvalid_1's rmse: 646.06\n",
      "[4400]\ttraining's rmse: 451.402\tvalid_1's rmse: 645.568\n",
      "[4600]\ttraining's rmse: 446.419\tvalid_1's rmse: 645.374\n",
      "[4800]\ttraining's rmse: 441.532\tvalid_1's rmse: 645.01\n",
      "[5000]\ttraining's rmse: 436.885\tvalid_1's rmse: 644.84\n",
      "[5200]\ttraining's rmse: 432.345\tvalid_1's rmse: 644.686\n",
      "[5400]\ttraining's rmse: 427.863\tvalid_1's rmse: 644.536\n",
      "Early stopping, best iteration is:\n",
      "[5334]\ttraining's rmse: 429.268\tvalid_1's rmse: 644.393\n",
      "\n",
      "[2019-08-06 19:22:05] Fold 2\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 897.935\tvalid_1's rmse: 903.116\n",
      "[400]\ttraining's rmse: 726.686\tvalid_1's rmse: 756.792\n",
      "[600]\ttraining's rmse: 669.091\tvalid_1's rmse: 715.131\n",
      "[800]\ttraining's rmse: 633.902\tvalid_1's rmse: 693.153\n",
      "[1000]\ttraining's rmse: 609.055\tvalid_1's rmse: 680.583\n",
      "[1200]\ttraining's rmse: 588.859\tvalid_1's rmse: 671.471\n",
      "[1400]\ttraining's rmse: 572.068\tvalid_1's rmse: 664.979\n",
      "[1600]\ttraining's rmse: 557.898\tvalid_1's rmse: 660.47\n",
      "[1800]\ttraining's rmse: 545.641\tvalid_1's rmse: 656.788\n",
      "[2000]\ttraining's rmse: 534.448\tvalid_1's rmse: 653.923\n",
      "[2200]\ttraining's rmse: 524.342\tvalid_1's rmse: 651.486\n",
      "[2400]\ttraining's rmse: 515.236\tvalid_1's rmse: 649.604\n",
      "[2600]\ttraining's rmse: 506.769\tvalid_1's rmse: 648.261\n",
      "[2800]\ttraining's rmse: 498.78\tvalid_1's rmse: 647.129\n",
      "[3000]\ttraining's rmse: 491.348\tvalid_1's rmse: 646.254\n",
      "[3200]\ttraining's rmse: 484.351\tvalid_1's rmse: 645.573\n",
      "[3400]\ttraining's rmse: 477.615\tvalid_1's rmse: 644.89\n",
      "[3600]\ttraining's rmse: 471.344\tvalid_1's rmse: 644.278\n",
      "[3800]\ttraining's rmse: 465.301\tvalid_1's rmse: 643.699\n",
      "[4000]\ttraining's rmse: 459.633\tvalid_1's rmse: 643.43\n",
      "[4200]\ttraining's rmse: 454.128\tvalid_1's rmse: 643.181\n",
      "[4400]\ttraining's rmse: 448.907\tvalid_1's rmse: 642.951\n",
      "[4600]\ttraining's rmse: 443.909\tvalid_1's rmse: 642.827\n",
      "[4800]\ttraining's rmse: 438.951\tvalid_1's rmse: 642.824\n",
      "[5000]\ttraining's rmse: 434.307\tvalid_1's rmse: 642.577\n",
      "Early stopping, best iteration is:\n",
      "[4924]\ttraining's rmse: 436.046\tvalid_1's rmse: 642.549\n",
      "\n",
      "[2019-08-06 19:23:16] Fold 3\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 888.473\tvalid_1's rmse: 941.22\n",
      "[400]\ttraining's rmse: 718.824\tvalid_1's rmse: 790.327\n",
      "[600]\ttraining's rmse: 662.182\tvalid_1's rmse: 749.219\n",
      "[800]\ttraining's rmse: 627.645\tvalid_1's rmse: 726.537\n",
      "[1000]\ttraining's rmse: 602.928\tvalid_1's rmse: 712.639\n",
      "[1200]\ttraining's rmse: 583.783\tvalid_1's rmse: 703.503\n",
      "[1400]\ttraining's rmse: 567.706\tvalid_1's rmse: 696.508\n",
      "[1600]\ttraining's rmse: 553.897\tvalid_1's rmse: 691.228\n",
      "[1800]\ttraining's rmse: 541.88\tvalid_1's rmse: 687.011\n",
      "[2000]\ttraining's rmse: 530.783\tvalid_1's rmse: 683.604\n",
      "[2200]\ttraining's rmse: 521.004\tvalid_1's rmse: 681.099\n",
      "[2400]\ttraining's rmse: 512.201\tvalid_1's rmse: 679.116\n",
      "[2600]\ttraining's rmse: 503.857\tvalid_1's rmse: 677.025\n",
      "[2800]\ttraining's rmse: 496.276\tvalid_1's rmse: 675.27\n",
      "[3000]\ttraining's rmse: 489.01\tvalid_1's rmse: 673.84\n",
      "[3200]\ttraining's rmse: 482.148\tvalid_1's rmse: 672.562\n",
      "[3400]\ttraining's rmse: 475.667\tvalid_1's rmse: 671.67\n",
      "[3600]\ttraining's rmse: 469.548\tvalid_1's rmse: 670.839\n",
      "[3800]\ttraining's rmse: 463.654\tvalid_1's rmse: 670.104\n",
      "[4000]\ttraining's rmse: 458.18\tvalid_1's rmse: 669.745\n",
      "[4200]\ttraining's rmse: 452.859\tvalid_1's rmse: 669.288\n",
      "[4400]\ttraining's rmse: 447.726\tvalid_1's rmse: 668.853\n",
      "[4600]\ttraining's rmse: 442.903\tvalid_1's rmse: 668.31\n",
      "[4800]\ttraining's rmse: 438.168\tvalid_1's rmse: 667.929\n",
      "[5000]\ttraining's rmse: 433.579\tvalid_1's rmse: 667.571\n",
      "[5200]\ttraining's rmse: 429.116\tvalid_1's rmse: 667.451\n",
      "[5400]\ttraining's rmse: 424.849\tvalid_1's rmse: 667.232\n",
      "[5600]\ttraining's rmse: 420.747\tvalid_1's rmse: 667.019\n",
      "[5800]\ttraining's rmse: 416.605\tvalid_1's rmse: 666.966\n",
      "[6000]\ttraining's rmse: 412.578\tvalid_1's rmse: 666.789\n",
      "[6200]\ttraining's rmse: 408.691\tvalid_1's rmse: 666.81\n",
      "Early stopping, best iteration is:\n",
      "[6017]\ttraining's rmse: 412.264\tvalid_1's rmse: 666.723\n",
      "\n",
      "[2019-08-06 19:24:39] Fold 4\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 894.627\tvalid_1's rmse: 921.441\n",
      "[400]\ttraining's rmse: 723.298\tvalid_1's rmse: 774.368\n",
      "[600]\ttraining's rmse: 666.197\tvalid_1's rmse: 734.145\n",
      "[800]\ttraining's rmse: 630.877\tvalid_1's rmse: 711.621\n",
      "[1000]\ttraining's rmse: 606.572\tvalid_1's rmse: 698.189\n",
      "[1200]\ttraining's rmse: 587.42\tvalid_1's rmse: 689.043\n",
      "[1400]\ttraining's rmse: 571.344\tvalid_1's rmse: 681.997\n",
      "[1600]\ttraining's rmse: 557.765\tvalid_1's rmse: 676.975\n",
      "[1800]\ttraining's rmse: 545.574\tvalid_1's rmse: 673.656\n",
      "[2000]\ttraining's rmse: 534.857\tvalid_1's rmse: 670.81\n",
      "[2200]\ttraining's rmse: 525.047\tvalid_1's rmse: 668.434\n",
      "[2400]\ttraining's rmse: 516.244\tvalid_1's rmse: 666.829\n",
      "[2600]\ttraining's rmse: 508.047\tvalid_1's rmse: 665.131\n",
      "[2800]\ttraining's rmse: 500.344\tvalid_1's rmse: 664.014\n",
      "[3000]\ttraining's rmse: 492.975\tvalid_1's rmse: 662.886\n",
      "[3200]\ttraining's rmse: 486.173\tvalid_1's rmse: 662.364\n",
      "[3400]\ttraining's rmse: 479.834\tvalid_1's rmse: 661.666\n",
      "[3600]\ttraining's rmse: 473.877\tvalid_1's rmse: 661.237\n",
      "[3800]\ttraining's rmse: 468.044\tvalid_1's rmse: 660.873\n",
      "[4000]\ttraining's rmse: 462.458\tvalid_1's rmse: 660.497\n",
      "[4200]\ttraining's rmse: 457.177\tvalid_1's rmse: 660.222\n",
      "[4400]\ttraining's rmse: 451.998\tvalid_1's rmse: 659.837\n",
      "[4600]\ttraining's rmse: 447.142\tvalid_1's rmse: 659.718\n",
      "[4800]\ttraining's rmse: 442.417\tvalid_1's rmse: 659.559\n",
      "[5000]\ttraining's rmse: 437.738\tvalid_1's rmse: 659.391\n",
      "[5200]\ttraining's rmse: 433.304\tvalid_1's rmse: 659.454\n",
      "Early stopping, best iteration is:\n",
      "[5064]\ttraining's rmse: 436.365\tvalid_1's rmse: 659.318\n",
      "\n",
      "[2019-08-06 19:25:51] Fold 5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 891.804\tvalid_1's rmse: 926.48\n",
      "[400]\ttraining's rmse: 724.509\tvalid_1's rmse: 770.778\n",
      "[600]\ttraining's rmse: 668.643\tvalid_1's rmse: 726.865\n",
      "[800]\ttraining's rmse: 633.997\tvalid_1's rmse: 703.804\n",
      "[1000]\ttraining's rmse: 609.455\tvalid_1's rmse: 690.577\n",
      "[1200]\ttraining's rmse: 589.823\tvalid_1's rmse: 681.86\n",
      "[1400]\ttraining's rmse: 573.411\tvalid_1's rmse: 675.171\n",
      "[1600]\ttraining's rmse: 559.814\tvalid_1's rmse: 670.377\n",
      "[1800]\ttraining's rmse: 547.412\tvalid_1's rmse: 666.696\n",
      "[2000]\ttraining's rmse: 536.346\tvalid_1's rmse: 663.547\n",
      "[2200]\ttraining's rmse: 526.439\tvalid_1's rmse: 661.217\n",
      "[2400]\ttraining's rmse: 517.167\tvalid_1's rmse: 659.483\n",
      "[2600]\ttraining's rmse: 508.82\tvalid_1's rmse: 657.722\n",
      "[2800]\ttraining's rmse: 501.098\tvalid_1's rmse: 656.706\n",
      "[3000]\ttraining's rmse: 493.738\tvalid_1's rmse: 655.487\n",
      "[3200]\ttraining's rmse: 486.848\tvalid_1's rmse: 654.982\n",
      "[3400]\ttraining's rmse: 480.219\tvalid_1's rmse: 654.293\n",
      "[3600]\ttraining's rmse: 474.034\tvalid_1's rmse: 654.04\n",
      "[3800]\ttraining's rmse: 467.975\tvalid_1's rmse: 653.819\n",
      "[4000]\ttraining's rmse: 462.221\tvalid_1's rmse: 653.312\n",
      "[4200]\ttraining's rmse: 456.971\tvalid_1's rmse: 653.221\n",
      "[4400]\ttraining's rmse: 451.74\tvalid_1's rmse: 653.112\n",
      "[4600]\ttraining's rmse: 446.774\tvalid_1's rmse: 653.029\n",
      "[4800]\ttraining's rmse: 441.942\tvalid_1's rmse: 652.898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[4739]\ttraining's rmse: 443.405\tvalid_1's rmse: 652.837\n",
      "K-Fold score:0.917418\n"
     ]
    }
   ],
   "source": [
    "#### K-Fold CV #####\n",
    "# 10-Fold CV\n",
    "import datetime\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "out_of_fold = np.zeros(len(X_train))\n",
    "predictions = np.zeros(len(X_test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "params = {\n",
    "    'num_leaves': 31,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'min_child_samples':20,\n",
    "    'objective': 'regression',\n",
    "    'learning_rate': 0.01,\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"bagging_fraction\": 0.85,\n",
    "    \"bagging_seed\": 23,\n",
    "    \"metric\": 'rmse',\n",
    "    \"lambda_l1\": 0.2,\n",
    "    \"nthread\": 4,\n",
    "}\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print( \"\\n[{}] Fold {}\".format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), fold+1))\n",
    "    trn_data = lgb.Dataset(X_train.iloc[train_idx], label=Y_train[train_idx])\n",
    "    val_data = lgb.Dataset(X_train.iloc[val_idx], label=Y_train[val_idx])\n",
    "    # Train\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(params, trn_data, num_round,valid_sets=[trn_data, val_data], verbose_eval=200,early_stopping_rounds=200)\n",
    "    out_of_fold[val_idx] = clf.predict(X_train.iloc[val_idx], num_iteration=clf.best_iteration)\n",
    "\n",
    "    \n",
    "    # Predict test data\n",
    "    predictions += clf.predict(X_test, num_iteration=clf.best_iteration) / kf.n_splits\n",
    "\n",
    "    # Feature Importance\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = X_train.columns\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "#     trn_data = lgb.Dataset(X_train.iloc[train_idx], label=Y_train[train_idx]/X_train.iloc[train_idx]['area'])\n",
    "#     val_data = lgb.Dataset(X_train.iloc[val_idx], label=Y_train[val_idx]/X_train.iloc[val_idx]['area'])\n",
    "\n",
    "#     # Train\n",
    "#     num_round = 10000\n",
    "#     clf = lgb.train(params, trn_data, num_round,valid_sets=[trn_data, val_data], verbose_eval=200,early_stopping_rounds=200)\n",
    "#     out_of_fold[val_idx] = (clf.predict(X_train.iloc[val_idx], num_iteration=clf.best_iteration))*X_train.iloc[val_idx]['area']\n",
    "\n",
    "    \n",
    "#     # Predict test data\n",
    "#     predictions += (clf.predict(X_test, num_iteration=clf.best_iteration) / kf.n_splits)*X_test['area']\n",
    "\n",
    "#     # Feature Importance\n",
    "#     fold_importance_df = pd.DataFrame()\n",
    "#     fold_importance_df[\"Feature\"] = X_train.columns\n",
    "#     fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "#     fold_importance_df[\"fold\"] = fold + 1\n",
    "#     feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "print('K-Fold score:{:.6f}'.format(r2_score(out_of_fold, Y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "def online_score(pred):\n",
    "    print(\"预测结果最大值：{},预测结果最小值：{}\".format(pred.max(),pred.min()))\n",
    "    # a榜测分\n",
    "    conmbine1 = pd.read_csv(\"./sub_b_919.csv\",engine = \"python\",header=None)\n",
    "    score1 = r2_score(pred, conmbine1)\n",
    "    print(\"对比919分数:{}\".format(score1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测结果最大值：15303.416250010467,预测结果最小值：1235.2221321561553\n",
      "对比919分数:0.9909407147885863\n"
     ]
    }
   ],
   "source": [
    "online_score(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- 预测结果最大值：15303.416250010467,预测结果最小值：1235.2221321561553\n",
    "  对比919分数:0.9909407147885863"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
